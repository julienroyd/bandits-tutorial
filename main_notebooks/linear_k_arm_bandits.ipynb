{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Random\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  3.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running UCB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:05<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running LinearUCBAgent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [2:45:46<1:51:47, 3353.87s/it]"
     ]
    }
   ],
   "source": [
    "class LinearMultiArmBandit:\n",
    "    \"\"\"\n",
    "    This class is what our agents will interact with.\n",
    "    In __init__:\n",
    "    - It instantiates the arms' reward distributions\n",
    "    - It sets the step counter to 0\n",
    "    - It initialises the reward, action, regret lists\n",
    "    In pull_arm:\n",
    "    - It increments the step counter\n",
    "    - It samples the reward from the arm's reward distribution\n",
    "    - It calculates the regret\n",
    "    - It returns the reward\n",
    "    \"\"\"\n",
    "    def __init__(self, k, d=8, reward_std=1., improve_best_action_n=10, sort_arms=True):\n",
    "        self.k = k\n",
    "        self.d = d\n",
    "        self.reward_std = reward_std\n",
    "        self.step = 0\n",
    "        self.reward_list = []\n",
    "        self.action_list = []\n",
    "        self.regret_list = []\n",
    "        self.arms = [np.random.normal(0, 1, d) for _ in range(k)]\n",
    "        self.true_theta = np.random.normal(0, 1, d)\n",
    "        if sort_arms:\n",
    "            self.arms = list(sorted(self.arms, key=lambda x: np.dot(self.true_theta, x)))\n",
    "        self.best_arm = np.argmax([self.get_mean_reward(arm) for arm in range(self.k)])\n",
    "        if improve_best_action_n is not None:\n",
    "            for _ in range(improve_best_action_n):\n",
    "                self.improve_best_action()\n",
    "\n",
    "    def pull_arm(self, arm):\n",
    "        self.step += 1\n",
    "        reward = self.get_reward(arm)\n",
    "        regret = self.get_regret(arm)\n",
    "        self.reward_list.append(reward)\n",
    "        self.action_list.append(arm)\n",
    "        self.regret_list.append(regret)\n",
    "        return reward\n",
    "\n",
    "    def improve_best_action(self):\n",
    "        # takes a few gradient steps to modify the best_arm's feature vector to make it even more dominant\n",
    "        # reward = theta @ x\n",
    "        # gradient_x = theta\n",
    "        # update on x: x = x + lr * theta\n",
    "        self.arms[self.best_arm] += 0.1 * self.true_theta\n",
    "\n",
    "    def get_mean_reward(self, arm):\n",
    "        return np.dot(self.true_theta, self.arms[arm])\n",
    "\n",
    "    def get_reward(self, arm):\n",
    "        return np.random.normal(np.dot(self.true_theta, self.arms[arm]), self.reward_std)\n",
    "\n",
    "    def get_regret(self, arm):\n",
    "        regret = np.dot(self.true_theta, self.arms[self.best_arm]) - np.dot(self.true_theta, self.arms[arm])\n",
    "        assert regret >= 0\n",
    "        return regret\n",
    "\n",
    "    def reset(self):\n",
    "        self.step = 0\n",
    "        self.reward_list = []\n",
    "        self.action_list = []\n",
    "        self.regret_list = []\n",
    "\n",
    "\n",
    "# -----------------\n",
    "\n",
    "\n",
    "class BanditAgent:\n",
    "    def select_arm(self):\n",
    "        raise NotImplementedError\n",
    "    def update(self, arm, reward):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class RandomAgent(BanditAgent):\n",
    "    # This agent chooses a random arm at each step\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "    def select_arm(self):\n",
    "        return np.random.randint(0, self.k)\n",
    "    def update(self, arm, reward):\n",
    "        pass\n",
    "    \n",
    "class QAgent(BanditAgent):\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "        self.q_values = np.zeros(k)\n",
    "        self.n_selections = np.zeros(k)\n",
    "    def select_arm(self):\n",
    "        raise NotImplementedError\n",
    "    def update(self, arm, reward):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class UCBAgent(QAgent):\n",
    "    # This agent selects the arm with the highest Upper Confidence Bound (UCB)\n",
    "    # The UCB is the sum of the q-value and a term that increases with the total \n",
    "    # number of steps and decreases with the number of times the arm has been selected\n",
    "    def __init__(self, k, c=1.):\n",
    "        super().__init__(k)\n",
    "        self.c = c\n",
    "    def select_arm(self):\n",
    "        if 0 in self.n_selections:\n",
    "            return np.argmin(self.n_selections)\n",
    "        ucb_values = self.q_values + self.c * np.sqrt(np.log(np.sum(self.n_selections)) / (self.n_selections))\n",
    "        return np.argmax(ucb_values)\n",
    "    def update(self, arm, reward):\n",
    "        self.n_selections[arm] += 1\n",
    "        self.q_values[arm] += (reward - self.q_values[arm]) / self.n_selections[arm]\n",
    "\n",
    "class LinearUCBAgent(QAgent):\n",
    "    def __init__(self, k, d=10, c=1.):\n",
    "        super().__init__(k)\n",
    "        self.d = d\n",
    "        self.c = c\n",
    "        self.arms = np.random.normal(0, 1, (k, d))\n",
    "        self.theta = np.zeros(d)\n",
    "        self.A = np.array([np.eye(d) for _ in range(k)])\n",
    "        self.b = np.zeros((k, d))\n",
    "    def select_arm(self):\n",
    "        ucb_values = np.array([np.dot(self.theta, self.arms[arm]) + self.c * np.sqrt(np.dot(self.arms[arm], np.linalg.solve(self.A[arm], self.arms[arm]))) for arm in range(self.k)])\n",
    "        return np.argmax(ucb_values)\n",
    "    def update(self, arm, reward):\n",
    "        self.A[arm] += np.outer(self.arms[arm], self.arms[arm])\n",
    "        self.b[arm] += self.arms[arm] * reward\n",
    "        self.theta = np.linalg.solve(self.A[arm], self.b[arm])\n",
    "\n",
    "class LinearQAgent(QAgent):\n",
    "    def __init__(self, k, arms, d=10, lr=0.1, temperature=0.1):\n",
    "        super().__init__(k)\n",
    "        self.d = d\n",
    "        self.lr = lr\n",
    "        self.arms = np.array(arms)\n",
    "        self.theta = np.zeros(d)\n",
    "        self.temperature = temperature\n",
    "    def softmax(self, x, temp):\n",
    "        x = x - np.max(x)\n",
    "        return np.exp(x / temp) / np.sum(np.exp(x / temp))\n",
    "    def select_arm(self):\n",
    "        preferences = np.dot(self.arms, self.theta)\n",
    "        return np.random.choice(np.arange(self.k), p=self.softmax(preferences, self.temperature))\n",
    "    def update(self, arm, reward):\n",
    "        self.theta += self.lr * (reward - np.dot(self.theta, self.arms[arm])) * self.arms[arm]\n",
    "        \n",
    "\n",
    "# -----------------\n",
    "\n",
    "def show_results(logs, bandit, n_episodes, ep_len, agents):\n",
    "    # plot reward distributions\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(35, 5))\n",
    "    ax.set_title('Reward distributions')\n",
    "    ax.set_xticks(np.arange(1, bandit.k + 1))\n",
    "    ax.set_xticklabels(np.arange(1, bandit.k + 1))\n",
    "    arm_means = [bandit.get_mean_reward(arm) for arm in range(bandit.k)]\n",
    "    arm_stds = [bandit.reward_std for arm in range(bandit.k)]\n",
    "    ax.errorbar(np.arange(1, bandit.k + 1), arm_means, yerr=arm_stds, fmt='o')\n",
    "    ax.set_xlabel('Arm')\n",
    "    ax.set_ylabel('Mean reward')\n",
    "    # limit xticks to 20 ticks\n",
    "    # make sure the first and final numbers are there\n",
    "    xticks = np.linspace(1, bandit.k, num=20, dtype=int)\n",
    "    xticks = np.unique(np.concatenate([xticks, [1, bandit.k]]))\n",
    "    ax.set_xticks(xticks)\n",
    "\n",
    "    # plot algorithm performance\n",
    "    fig, ax = plt.subplots(1, 4, figsize=(35, 5))\n",
    "\n",
    "    all_colors = ['blue', 'orange', 'green', 'red', 'purple', 'brown', 'pink', 'gray', 'olive', 'cyan']\n",
    "    agent_colors = {agent_name: color for agent_name, color in zip(agents.keys(), all_colors)}\n",
    "\n",
    "    for agent_name, agent_logs in logs.items():\n",
    "        for i, metric in enumerate(agent_logs.keys()):\n",
    "            color = agent_colors[agent_name]\n",
    "            mean = np.mean(agent_logs[metric], axis=0)\n",
    "            ste = np.std(agent_logs[metric], axis=0) / np.sqrt(n_episodes)\n",
    "            ax[i].plot(mean, label=agent_name, color=color)\n",
    "            ax[i].set_xlabel('Timestep')\n",
    "            ax[i].set_title(metric)\n",
    "            if 'Action' in metric:\n",
    "                ax[i].axhline(y=bandit.best_arm, color='black', linestyle='--')\n",
    "            ax[i].fill_between(range(ep_len), mean - ste, mean + ste, alpha=0.5, color=color)\n",
    "        \n",
    "    ax[-1].legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "\n",
    "\n",
    "# ------------\n",
    "\n",
    "k = 10_000\n",
    "d = 16\n",
    "reward_std = 5.\n",
    "n_episodes = 5\n",
    "ep_len = 20_000\n",
    "bandit = LinearMultiArmBandit(k, d=d, reward_std=reward_std, improve_best_action_n=0)\n",
    "\n",
    "agents = {\n",
    "    'Random': RandomAgent,\n",
    "    'UCB': lambda k: UCBAgent(k, c=0.01),\n",
    "    'LinearUCBAgent': lambda k: LinearUCBAgent(k, d=bandit.d, c=1.),\n",
    "    'LinearQAgent': lambda k: LinearQAgent(k, arms=bandit.arms, d=bandit.d, lr=0.01, temperature=1.),\n",
    "}\n",
    "\n",
    "logs = {\n",
    "    agent_name: {\n",
    "        'Reward at timestep t': [],\n",
    "        'Action at timestep t': [],\n",
    "        'Regret at timestep t': [],\n",
    "        'Cumulative Regret': [],\n",
    "    } for agent_name in agents.keys()\n",
    "}\n",
    "\n",
    "for agent_name, Algo in agents.items():\n",
    "    print(f'Running {agent_name}')\n",
    "    for _ in tqdm(range(n_episodes)):\n",
    "        agent = Algo(bandit.k)\n",
    "        for _ in range(ep_len):\n",
    "            arm = agent.select_arm()\n",
    "            reward = bandit.pull_arm(arm)\n",
    "            agent.update(arm, reward)\n",
    "        logs[agent_name]['Reward at timestep t'].append(deepcopy(bandit.reward_list))\n",
    "        logs[agent_name]['Action at timestep t'].append(deepcopy(bandit.action_list))\n",
    "        logs[agent_name]['Regret at timestep t'].append(deepcopy(bandit.regret_list))\n",
    "        logs[agent_name]['Cumulative Regret'].append(np.cumsum(bandit.regret_list))\n",
    "        bandit.reset()\n",
    "\n",
    "show_results(logs, bandit, n_episodes, ep_len, agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gfn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
